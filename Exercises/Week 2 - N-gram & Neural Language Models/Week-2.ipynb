{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3275ace2",
   "metadata": {},
   "source": [
    "# 📚  Exercise Session - Week 2\n",
    "\n",
    "Welcome to Week 2 exercise's session of CS552-Modern NLP!\n",
    "\n",
    "\n",
    "> **What will be covered:**\n",
    "1. [**TASK A:** N-gram Language Models](#ngram_lm)\n",
    "    - [Unigram Language Model](#unigram_lm)\n",
    "    - [Bi-gram Language Model](#bigram_lm)\n",
    "    - [Tri-gram Language Model](#trigram_lm)\n",
    "     \n",
    "2. [**TASK B:** Neural Language Models](#neural_lm)\n",
    "    - [Fixed-Window Neural Language Model](#fixed_window_lm)\n",
    "    - [RNN-based Language Model](#rnn_lm)\n",
    "\n",
    "> **By the end of the session you will be able to:**\n",
    "> - ✅  Compute and interpret the perplexity of a language model \n",
    "> - ✅  Implement N-gram language models for N=1,2,3\n",
    "> - ✅  Implement, train, and evaluate a fixed window language model\n",
    "> - ✅  Evaluate an RNN language model\n",
    "> - ✅  Understand the advantages and disadvantages of each of the above models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98eb6b6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /home/xinyu/anaconda3/lib/python3.9/site-packages (from -r requirements.txt (line 1)) (1.13.0)\n",
      "Requirement already satisfied: numpy in /home/xinyu/anaconda3/lib/python3.9/site-packages (from -r requirements.txt (line 2)) (1.21.5)\n",
      "Collecting datasets (from -r requirements.txt (line 3))\n",
      "  Downloading datasets-2.18.0-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: typing_extensions in /home/xinyu/anaconda3/lib/python3.9/site-packages (from torch->-r requirements.txt (line 1)) (4.3.0)\n",
      "Requirement already satisfied: filelock in /home/xinyu/anaconda3/lib/python3.9/site-packages (from datasets->-r requirements.txt (line 3)) (3.6.0)\n",
      "Collecting pyarrow>=12.0.0 (from datasets->-r requirements.txt (line 3))\n",
      "  Downloading pyarrow-15.0.0-cp39-cp39-manylinux_2_28_x86_64.whl.metadata (3.0 kB)\n",
      "Collecting pyarrow-hotfix (from datasets->-r requirements.txt (line 3))\n",
      "  Downloading pyarrow_hotfix-0.6-py3-none-any.whl.metadata (3.6 kB)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /home/xinyu/anaconda3/lib/python3.9/site-packages (from datasets->-r requirements.txt (line 3)) (0.3.4)\n",
      "Requirement already satisfied: pandas in /home/xinyu/anaconda3/lib/python3.9/site-packages (from datasets->-r requirements.txt (line 3)) (1.4.4)\n",
      "Requirement already satisfied: requests>=2.19.0 in /home/xinyu/anaconda3/lib/python3.9/site-packages (from datasets->-r requirements.txt (line 3)) (2.28.1)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /home/xinyu/anaconda3/lib/python3.9/site-packages (from datasets->-r requirements.txt (line 3)) (4.64.1)\n",
      "Collecting xxhash (from datasets->-r requirements.txt (line 3))\n",
      "  Downloading xxhash-3.4.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multiprocess (from datasets->-r requirements.txt (line 3))\n",
      "  Downloading multiprocess-0.70.16-py39-none-any.whl.metadata (7.2 kB)\n",
      "Collecting fsspec<=2024.2.0,>=2023.1.0 (from fsspec[http]<=2024.2.0,>=2023.1.0->datasets->-r requirements.txt (line 3))\n",
      "  Downloading fsspec-2024.2.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: aiohttp in /home/xinyu/anaconda3/lib/python3.9/site-packages (from datasets->-r requirements.txt (line 3)) (3.8.1)\n",
      "Collecting huggingface-hub>=0.19.4 (from datasets->-r requirements.txt (line 3))\n",
      "  Downloading huggingface_hub-0.21.3-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: packaging in /home/xinyu/anaconda3/lib/python3.9/site-packages (from datasets->-r requirements.txt (line 3)) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/xinyu/anaconda3/lib/python3.9/site-packages (from datasets->-r requirements.txt (line 3)) (6.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/xinyu/anaconda3/lib/python3.9/site-packages (from aiohttp->datasets->-r requirements.txt (line 3)) (21.4.0)\n",
      "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /home/xinyu/anaconda3/lib/python3.9/site-packages (from aiohttp->datasets->-r requirements.txt (line 3)) (2.0.4)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/xinyu/anaconda3/lib/python3.9/site-packages (from aiohttp->datasets->-r requirements.txt (line 3)) (6.0.2)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /home/xinyu/anaconda3/lib/python3.9/site-packages (from aiohttp->datasets->-r requirements.txt (line 3)) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/xinyu/anaconda3/lib/python3.9/site-packages (from aiohttp->datasets->-r requirements.txt (line 3)) (1.8.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/xinyu/anaconda3/lib/python3.9/site-packages (from aiohttp->datasets->-r requirements.txt (line 3)) (1.2.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/xinyu/anaconda3/lib/python3.9/site-packages (from aiohttp->datasets->-r requirements.txt (line 3)) (1.2.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/xinyu/anaconda3/lib/python3.9/site-packages (from packaging->datasets->-r requirements.txt (line 3)) (3.0.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/xinyu/anaconda3/lib/python3.9/site-packages (from requests>=2.19.0->datasets->-r requirements.txt (line 3)) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/xinyu/anaconda3/lib/python3.9/site-packages (from requests>=2.19.0->datasets->-r requirements.txt (line 3)) (1.26.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/xinyu/anaconda3/lib/python3.9/site-packages (from requests>=2.19.0->datasets->-r requirements.txt (line 3)) (2022.9.14)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets->-r requirements.txt (line 3))\n",
      "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /home/xinyu/anaconda3/lib/python3.9/site-packages (from pandas->datasets->-r requirements.txt (line 3)) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/xinyu/anaconda3/lib/python3.9/site-packages (from pandas->datasets->-r requirements.txt (line 3)) (2022.1)\n",
      "Requirement already satisfied: six>=1.5 in /home/xinyu/anaconda3/lib/python3.9/site-packages (from python-dateutil>=2.8.1->pandas->datasets->-r requirements.txt (line 3)) (1.16.0)\n",
      "Downloading datasets-2.18.0-py3-none-any.whl (510 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m510.5/510.5 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading fsspec-2024.2.0-py3-none-any.whl (170 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m170.9/170.9 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.21.3-py3-none-any.whl (346 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m346.2/346.2 kB\u001b[0m \u001b[31m32.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyarrow-15.0.0-cp39-cp39-manylinux_2_28_x86_64.whl (38.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.3/38.3 MB\u001b[0m \u001b[31m31.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading multiprocess-0.70.16-py39-none-any.whl (133 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.4/133.4 kB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\n",
      "Downloading xxhash-3.4.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (193 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.8/193.8 kB\u001b[0m \u001b[31m24.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: xxhash, pyarrow-hotfix, pyarrow, fsspec, dill, multiprocess, huggingface-hub, datasets\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2022.7.1\n",
      "    Uninstalling fsspec-2022.7.1:\n",
      "      Successfully uninstalled fsspec-2022.7.1\n",
      "  Attempting uninstall: dill\n",
      "    Found existing installation: dill 0.3.4\n",
      "    Uninstalling dill-0.3.4:\n",
      "      Successfully uninstalled dill-0.3.4\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "spyder 5.3.3 requires pyqt5<5.16, which is not installed.\n",
      "spyder 5.3.3 requires pyqtwebengine<5.16, which is not installed.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed datasets-2.18.0 dill-0.3.8 fsspec-2024.2.0 huggingface-hub-0.21.3 multiprocess-0.70.16 pyarrow-15.0.0 pyarrow-hotfix-0.6 xxhash-3.4.1\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# install the libraries if needed using the requirements.txt file\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "272c0a7c",
   "metadata": {},
   "source": [
    "<a name=\"ngram_lm\"></a>\n",
    "## 1. Task A: N-gram Language Models "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c137c6",
   "metadata": {},
   "source": [
    "\n",
    "In this exercise, we will better understand the functioning of different types of (non-neural) language modeling, namely,  Unigram LM, Bi-gram LM, and Tri-gram LM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "066c0427",
   "metadata": {},
   "source": [
    "### 1.1 Unigram Language Model <a name=\"unigram_lm\"></a>\n",
    "In the simple Unigram language model, we pick/generate next token independent of the previous token. In other words, during the generation, we pick the tokens according to the token probability. Therefore, for an arbitrary sequence $x_1x_2~...x_n$, its respective probability becomes:\n",
    "$$p(x_1x_2~...x_n) = \\Pi_{i=1} ^n p(x_i)$$\n",
    "Let's use an unsupervised dataset (raw corpus) to evaluate this model's perplexity. We use Huggingface's `datasets` library to download needed datasets.\n",
    " \n",
    "\n",
    "Here we use the `Penn Treebank` dataset, featuring a million words of 1989 Wall Street Journal material. The rare words in this version are already replaced with `<unk>` token. The numbers are also replaced with a special token. This token replacement helps us to end up with a more reasonable vocabulary size to work with.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "69afb9de",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 2.96M/2.96M [00:00<00:00, 9.68MB/s]\n",
      "Downloading data: 100%|██████████| 262k/262k [00:00<00:00, 682kB/s]\n",
      "Downloading data: 100%|██████████| 236k/236k [00:00<00:00, 1.91MB/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61fbd42fe69c4b1dbc291ae9edfe8f63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/42068 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2faf63d454064c0cb54a1d3d787e9e92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/3761 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "442f23a060ce4ded8402f66cfcc8d678",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/3370 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import datasets\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "\n",
    "ptb_dataset = load_dataset(\"ptb_text_only\", split=\"train\")\n",
    "\n",
    "# splitting dataset in train/test (to be later used for language model evaluation)\n",
    "ptb_dataset = ptb_dataset.train_test_split(test_size=0.2, seed=1)\n",
    "ptb_train, ptb_test = ptb_dataset['train'], ptb_dataset['test']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20589cd0",
   "metadata": {},
   "source": [
    "#### Let's have a look at a few samples of the training dataset (and also the structure of the dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b6ceb684",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sentence': \"a former executive agreed that the departures do n't reflect major problems adding if you see any company that grows as fast as reebok did it is going to have people coming and going\"}\n",
      "\n",
      "{'sentence': 'with talk today of a second economic <unk> in west germany east germany no longer can content itself with being the economic star in a loser league'}\n",
      "\n",
      "{'sentence': 'transportation secretary sam skinner who earlier fueled the anti-takeover fires with his <unk> attacks on foreign investment in u.s. carriers now says the bill would further <unk> the jittery capital markets'}\n"
     ]
    }
   ],
   "source": [
    "print(f\"{ptb_train[0]}\\n\\n{ptb_train[1]}\\n\\n{ptb_train[2]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07207b13",
   "metadata": {},
   "source": [
    "During generation with a given language model, we often need to have a `<stop>` token in our vocabulary to terminate the generation of a given sentence/paragraph. In this dataset, every sample is a sentence, and the `<stop>` token should be added to the end of every sample (i.e., end of sentence).\n",
    "\n",
    "#### Create a new train/test dataset starting from `ptb_train` and `ptb_test` that has a `<stop>` at the end of each sentence. (Note: do not change the structure of the datasets objects, and just change the respective sentences as discussed).\n",
    "Hint: use the `.map()` functionality of the `datasets` package (read more [here](https://huggingface.co/docs/datasets/process#map]))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aa1b7b70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2771d9a5ae8d419c954e7b7ad523426f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/33654 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0dbc1e767d554c9baf35bcafb059b086",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/8414 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def add_stop_token(input_sample: dict):\n",
    "    '''\n",
    "    args:\n",
    "        input_sample: a dict representing a sample of the dataset. (look above for the dict struture)\n",
    "    output:\n",
    "        modified_sample: modified dict adding <stop> at the end of each sentence.\n",
    "    '''\n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    modified_sample = input_sample\n",
    "    modified_sample['sentence'] += ' <stop>'\n",
    "\n",
    "    return modified_sample\n",
    "    \n",
    "    \n",
    "ptb_train = ptb_train.map(add_stop_token)\n",
    "ptb_test = ptb_test.map(add_stop_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d338821",
   "metadata": {},
   "source": [
    "For both `ptb_train` and `ptb_test` datasets, filter out every sample that has less than 3 tokens. it will help remove very short sentences that are not very helpful for training/evaluating a langugage model.\n",
    "\n",
    "Hint: use `.filter()` functionality of the `datasets` package (read more [here](https://huggingface.co/docs/datasets/process#select-and-filter))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9c0222f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "ptb_train = ptb_train.filter(lambda example : len(example['sentence']) > 4)\n",
    "ptb_test = ptb_test.filter(lambda example : len(example['sentence']) > 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2733f77b",
   "metadata": {},
   "source": [
    "#### What are the 10 most frequent tokens in this dataset? Can you spot the token used to replace the numbers in this dataset? How are rare tokens replaced in this dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "28e93fb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 40616),\n",
       " ('<unk>', 35918),\n",
       " ('<stop>', 33654),\n",
       " ('N', 25966),\n",
       " ('of', 19459),\n",
       " ('to', 18896),\n",
       " ('a', 16901),\n",
       " ('in', 14473),\n",
       " ('and', 14013),\n",
       " (\"'s\", 7850)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "from collections import Counter\n",
    "Counter(\" \".join([i[\"sentence\"] for i in ptb_train]).split()).most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "81515b27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16901"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frequencies = Counter(\" \".join([i[\"sentence\"] for i in ptb_train]).split())\n",
    "frequencies['a']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6eaec09",
   "metadata": {},
   "source": [
    "#### Now let's create a dictionary of the word probabilites (in the format of `{word: Prob(word)}`in the following function. We will use these probabilities to estimate sequence probabilities for a given sequence, as mentioned above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0e015a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict, Counter\n",
    "\n",
    "def get_word_probability_dict(train_dataset: datasets.arrow_dataset.Dataset):\n",
    "    '''\n",
    "    args: \n",
    "        train_dataset: a Dataset object that can be iterated to get all the sentences\n",
    "    output:\n",
    "        word_prob_dict: a dictionary containing the word probabilities (and outputing zero for non-seen tokens)\n",
    "    '''\n",
    "    # YOUR CODE HERE\n",
    "\n",
    "    word_prob_dict = Counter(\" \".join([i[\"sentence\"] for i in train_dataset]).split())\n",
    "    total = sum(frequencies.values())\n",
    "\n",
    "    word_prob_dict = dict((k, v / total) for k, v in word_prob_dict.items())\n",
    "\n",
    "    return word_prob_dict\n",
    "\n",
    "word_prob_dict = get_word_probability_dict(ptb_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "766d403e",
   "metadata": {},
   "source": [
    "Let's also get a sense of how high the top-k probabilities are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "37d958f6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 0.05466398927608168),\n",
       " ('<unk>', 0.04834107659095681),\n",
       " ('<stop>', 0.04529401947747816),\n",
       " ('N', 0.034946945675170794),\n",
       " ('of', 0.026189348220486346),\n",
       " ('to', 0.025431621561966697),\n",
       " ('a', 0.022746604361706137),\n",
       " ('in', 0.01947882402975995),\n",
       " ('and', 0.01885972231942418),\n",
       " (\"'s\", 0.010565105274208222),\n",
       " ('for', 0.00956646729797096),\n",
       " ('that', 0.009555700311704253),\n",
       " ('$', 0.008017367148848202),\n",
       " ('is', 0.007972953330498026),\n",
       " ('it', 0.006554402889859114),\n",
       " ('said', 0.006514026691358955),\n",
       " ('on', 0.00604835453532379),\n",
       " ('at', 0.005310815976054223),\n",
       " ('by', 0.005301394863070852),\n",
       " ('as', 0.0051950708736871005)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(word_prob_dict.items(), key=lambda item: item[1], reverse=True)[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1b408c2",
   "metadata": {},
   "source": [
    "#### Now let's analyze the Unigram language model for different sequences. We first create a function that can output the probability for a given string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fc167228",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unigram_lm_seq_probability(input_sentence: str,\n",
    "                               word_prob_dict: dict):\n",
    "    '''\n",
    "    args:\n",
    "        input_sentence: The input sequence string. Here we assume\n",
    "        word_prob_dict: A dictionary containing the probability for a given token\n",
    "    output:\n",
    "        probability: The probability of the input_sentence according to the Unigram language model\n",
    "    '''\n",
    "    # YOUR CODE HERE\n",
    "    unigram = [word_prob_dict[i] for i in input_sentence.split()]\n",
    "    probability = 1\n",
    "    for i in unigram:\n",
    "        probability *= i\n",
    "    \n",
    "    return probability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3427e046",
   "metadata": {},
   "source": [
    "#### Let's investigate a major issue with Unigram language model. What are the probabilities for the two following sequences?\n",
    "- the the the the \\<stop>\n",
    "- i love computer science \\<stop>\n",
    "\n",
    "DIscussion: How can we avoid having large probability values for sequences like `the the the <stop>`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2c753d2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "probability for seq1 is 4.0443259736782917e-07, and for seq2 is 2.3529893814050696e-17\n"
     ]
    }
   ],
   "source": [
    "seq1 = \"the the the the <stop>\"\n",
    "seq2 = \"i love computer science <stop>\"\n",
    "\n",
    "prob_seq1 = unigram_lm_seq_probability(seq1, word_prob_dict)\n",
    "prob_seq2 = unigram_lm_seq_probability(seq2, word_prob_dict)\n",
    "print(f\"probability for seq1 is {prob_seq1}, and for seq2 is {prob_seq2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06747969",
   "metadata": {},
   "source": [
    "#### Now let's formally evaluate the Unigram model in terms of perplexity. We first compute the entropy as the average negative log-likelihood:\n",
    "$$H(W_{test}∣M)= \\frac{1}{|W_{test}|} \\sum_{w\\in W_{test}} −log_2P(w∣M)$$\n",
    ", where $W_{test}$ is the input sequence and M is the Unigram language model. (note that the logarithm is in base 2).\n",
    "\n",
    "In order to get a reliable value, we will do the above calculation for all the sentences in `ptb_test` dataset and then an average is taken over all these samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "aa48300f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unigram_lm_entropy(input_sentence: str,\n",
    "                           word_prob_dict: dict):\n",
    "    '''\n",
    "    args:\n",
    "        input_sentence: the input string that we would like to have its respective entropy value.\n",
    "        word_prob_dict: A dictionary containing the probability for a given token\n",
    "    output:\n",
    "        entropy: entropy value as defined above\n",
    "    '''\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    tokens = input_sentence.split()\n",
    "    entropy = np.mean([-np.log2(word_prob_dict[i]) for i in tokens if i in word_prob_dict])\n",
    "\n",
    "    return entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b07afa63",
   "metadata": {},
   "source": [
    "Now compute the average entropy for all the sentences in the `ptb_test` given above function, and then compute the average entropy. Then compute the perplexity as $2^{\\bar{H}}$, where $\\bar{H}$ is the average entropy over the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "ac66be2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The perplexity for the Unigram language model is 675.1906049051033\n"
     ]
    }
   ],
   "source": [
    "def get_unigram_lm_perplexity(test_dataset: datasets.arrow_dataset.Dataset,\n",
    "                              word_prob_dict: dict):\n",
    "    '''\n",
    "    args:\n",
    "        test_dataset: the test dataset samples are used to compute the perplexity for the Unigram LM.\n",
    "        word_prob_dict: A dictionary containing the probability for a given token\n",
    "    output:\n",
    "        perplexity: entropy value as defined above\n",
    "    '''  \n",
    "    # YOUR CODE HERE\n",
    "    entropy_values = [get_unigram_lm_entropy(i['sentence'], word_prob_dict) for i in test_dataset]\n",
    "    average_entropy = np.mean([entropy for entropy in entropy_values])\n",
    "    perplexity = 2 ** average_entropy\n",
    "    return perplexity\n",
    "      \n",
    "unigram_lm_perplexity = get_unigram_lm_perplexity(ptb_test, word_prob_dict)\n",
    "print(f\"The perplexity for the Unigram language model is {unigram_lm_perplexity}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a3cec0a",
   "metadata": {},
   "source": [
    "As discussed in the lectures, the models with lower perplexities are desired; however, we should be careful when comparing language models with different vocabualry sizes.\n",
    "#### In the `ptb_train` dataset, replace every token that is appearing less than 10 times with the `<unk>` token. (Note: the same token replacement should be done for the test dataset). What is the Unigram language model perplexity for the new dataset?\n",
    "Discussion: What would happen to the vocabulary size and perplexity as we increase the rare token threshold to higher values? (instead of 10 here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "3a14c853",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90875dd3881443b8b27bb7d1928c2c43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/33654 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efb486566bcf442591170e53ffc3ae0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/8414 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def remove_rare_token(train_dataset: datasets.arrow_dataset.Dataset,\n",
    "                      test_dataset: datasets.arrow_dataset.Dataset,\n",
    "                      rare_token_threshold: int):\n",
    "    '''\n",
    "    Note that the tokens that are considered rare here, are identified based on the train_dataset, so that\n",
    "    we have the same token mapping (to <unk>) for both the train and test datasets. \n",
    "    args:\n",
    "        train_dataset: the input dataset where its rare tokens has to be replaced with <unk> token.\n",
    "        rare_token_threshold: every word that is appearing less than this threshold in the train dataset will\n",
    "                              be replace with the <unk> token\n",
    "    output:\n",
    "        cleaned_train_dataset: the cleaned train dataset where rare tokens are replace with <unk> token.\n",
    "        cleaned_test_dataset: the cleaned test dataset where rare tokens are replace with <unk> token.\n",
    "    '''\n",
    "\n",
    "    word_count = Counter(\" \".join([i[\"sentence\"] for i in train_dataset]).split())\n",
    "\n",
    "    def remove_rare_token_sentence(entry):\n",
    "        sentence = entry[\"sentence\"].split()\n",
    "        sentence = list(map(lambda x : '<unk>' if word_count[x] < rare_token_threshold else x, sentence))\n",
    "        entry[\"sentence\"] = \" \".join(sentence)\n",
    "        return entry\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    cleaned_train_dataset = train_dataset.map(remove_rare_token_sentence)\n",
    "    cleaned_test_dataset = test_dataset.map(remove_rare_token_sentence)\n",
    "    \n",
    "    return cleaned_train_dataset, cleaned_test_dataset\n",
    "\n",
    "cleaned_train_dataset, cleaned_test_dataset = remove_rare_token(train_dataset=ptb_train,\n",
    "                                                                test_dataset=ptb_test,\n",
    "                                                                rare_token_threshold=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f09a7d3",
   "metadata": {},
   "source": [
    "##### Now, follow similar steps to compute the perplexity given the two new datasets (`cleaned_train_dataset` and `cleaned_test_dataset`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "d2083f50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The perplexity for the Unigram language model after replacing rare tokens is  457.9811721064766\n"
     ]
    }
   ],
   "source": [
    "cleaned_unigram_lm_perplexity = -1\n",
    "\n",
    "# YOUR CODE HERE\n",
    "cleaned_word_prob_dict = get_word_probability_dict(cleaned_train_dataset)\n",
    "\n",
    "cleaned_unigram_lm_perplexity = get_unigram_lm_perplexity(cleaned_test_dataset, cleaned_word_prob_dict)\n",
    "\n",
    "print(\"The perplexity for the Unigram language model after replacing rare tokens is \",\n",
    "      cleaned_unigram_lm_perplexity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4abe9eac",
   "metadata": {},
   "source": [
    "## 1.2 Bi-gram Language Model <a name='bigram_lm'></a>\n",
    "In the Bi-gram language model, we pick/generate next token conditioned only on the previous token. Therefore, for an arbitrary sequence $x_1x_2~...x_n$, its respective probability becomes:\n",
    "$$p(x_1x_2~...x_n) = p(x_1) ~\\Pi_{i=2} ^n p(x_i|x_{i-1})$$\n",
    "Let's use the same dataset (`Penn Treebank`) to evaluate this model's perplexity. (We use the dataset that already has the `<stop>` token at the end).\n",
    "\n",
    "We estimate $p(x_i|x_{i-1})$ as the $\\frac{count(x_{i-1},~x_i)}{count(x_{i-1})}$ according to the training dataset frequencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "18678df2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'former'"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = \" \".join([i[\"sentence\"] for i in ptb_train]).split()\n",
    "bigram = []\n",
    "for i in range(len(test) - 1):\n",
    "    bigram.append((test[i], test[i + 1]))\n",
    "bigram_count = Counter(bigram)\n",
    "\n",
    "# total = sum(bigram_count.values())\n",
    "\n",
    "# bigram_prob_dict = dict((k, v / total) for k, v in bigram_count.items())\n",
    "\n",
    "test = ('a', 'former')\n",
    "test[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "f96966fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_first_order_conditional_probabilities(train_dataset: datasets.arrow_dataset.Dataset):\n",
    "    '''\n",
    "    In this function the conditional probabilities have to be computed based train_dataset. The output of the\n",
    "    function is a dictionary having keys like (x_{i-1}, x_i) as a tuple and the value being p(x_i|x_{i-1}).\n",
    "    args:\n",
    "        train_dataset: a Dataset object that can be iterated to get all the sentences\n",
    "    output:\n",
    "        word_prob_dict: \n",
    "        first_order_condition_prob: a dictionary having containing the first order conditional probabilities\n",
    "                                    as discussed above.\n",
    "        word_prob_dict: a dictionary containing the word probabilities\n",
    "    '''\n",
    "    first_order_condition_prob = defaultdict(float) # in order to get zeroes \n",
    "    # let's first get the word frequencies (later used for computation of conditional probabilities)\n",
    "    word_prob_dict = get_word_probability_dict(train_dataset)\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    all_tokens = \" \".join([i[\"sentence\"] for i in ptb_train]).split()\n",
    "    word_frequency = Counter(all_tokens)\n",
    "    bigram = []\n",
    "    for i in range(len(all_tokens) - 1):\n",
    "        bigram.append((all_tokens[i], all_tokens[i + 1]))\n",
    "    bigram_count = Counter(bigram)\n",
    "\n",
    "    for i in bigram:\n",
    "        first_order_condition_prob[i] = bigram_count[i] / word_frequency[i[0]]\n",
    "\n",
    "    return word_prob_dict, first_order_condition_prob\n",
    "\n",
    "word_prob_dict, first_order_condition_prob = get_first_order_conditional_probabilities(ptb_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c89cb406",
   "metadata": {},
   "source": [
    "#### Now let's analyze the Bi-gram language model for different sequences. We first create a function that can output the probability for a given string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "e459cdc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bigram_lm_seq_probability(input_sentence: str,\n",
    "                              word_prob_dict: dict,\n",
    "                              first_order_condition_prob: dict):\n",
    "    '''\n",
    "    args:\n",
    "        input_sentence: The input sequence string. Here we assume\n",
    "        word_prob_dict: a dictionary containing the word probabilities\n",
    "        first_order_condition_prob: a dictionary containing the first order conditional probabilities\n",
    "                                    as discussed in the previous function.\n",
    "    output:\n",
    "        probability: The probability of the input_sentence according to the Bi-gram language model\n",
    "    '''\n",
    "    # YOUR CODE HERE\n",
    "    tokens = input_sentence.split()\n",
    "    bigram = [(tokens[i], tokens[i + 1]) for i in range(len(tokens) - 1)]\n",
    "    \n",
    "    probability = word_prob_dict[tokens[0]] * np.prod([first_order_condition_prob[i] for i in bigram])\n",
    "\n",
    "    return probability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a87509ef",
   "metadata": {},
   "source": [
    "Let's investigate a major issue with higher order language models.\n",
    "#### Compute the probabilities for all the sequences in `ptb_test` dataset, and compute the minimum value among these probablities. What would be the perplexity for the dataset given these values?\n",
    "Discussion: How can we avoid this **overfitting** to train dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "3d9a0460",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90.62277157119087% of samples in the test set have zero probability!\n"
     ]
    }
   ],
   "source": [
    "bigram_test_probabilities = []\n",
    "\n",
    "# YOUR CODE HERE\n",
    "bigram_test_probabilities = [bigram_lm_seq_probability(i['sentence'], word_prob_dict, first_order_condition_prob) for i in ptb_test]\n",
    "\n",
    "print(f\"{bigram_test_probabilities.count(0)/len(ptb_test)*100}% of samples in the test set have zero probability!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ced914f",
   "metadata": {},
   "source": [
    "### Smoothing\n",
    "As we saw above, due to having new pair of consecutive words in the test dataset, we might have zero probabilities for some sequences. Therefore, as discussed in the lectures, in order to have a meaningful perplexity for N-gram language models, we need to smooth the probabilities to have non-zero values for non-seen sequences. In this exercise, we use Laplace smoothing as defined below:\n",
    "$$P(x_i|x_{i-1}) = \\frac{count(x_{i-1},~x_i) + \\alpha}{count(x_{i-1}) + \\alpha ~|V|}$$\n",
    ", where $\\alpha$ is the smoothing parameter, and $|V|$ is the (train dataset) vocabulary size.\n",
    "\n",
    "#### Let's recompute the conditional probabilities using Laplace smoothing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "5d808944",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_smoothed_first_order_conditional_probabilities(train_dataset: datasets.arrow_dataset.Dataset,\n",
    "                                                       smoothing_alpha: float):\n",
    "    '''\n",
    "    In this function the conditional probabilities have to be computed based on train_dataset. The output\n",
    "    of the function is a dictionary having keys like (x_{i-1}, x_i) as a tuple and the\n",
    "    value being p(x_i|x_{i-1}).\n",
    "    args:\n",
    "        train_dataset: a Dataset object that can be iterated to get all the sentences\n",
    "        smoothing_alpha: The alpha parameter used in the Laplace smoothing.\n",
    "    output:\n",
    "        word_prob_dict: a dictionary containing the word probabilities \n",
    "        first_order_condition_prob: a dictionary containing the smoothed first order\n",
    "                                    conditional probabilities as discussed above.\n",
    "    '''\n",
    "    first_order_condition_prob = defaultdict(float)  # Note that we shouldn't get zeros for unseen events.\n",
    "    # let's first get the word probabilities (later used for computation of conditional probabilities)\n",
    "    word_prob_dict = get_word_probability_dict(train_dataset)\n",
    "    \n",
    "    # YOUR CODE here\n",
    "    all_tokens = \" \".join([i[\"sentence\"] for i in ptb_train]).split()\n",
    "    \n",
    "    word_frequency = Counter(all_tokens)\n",
    "    V = len(word_prob_dict)\n",
    "\n",
    "    bigram = []\n",
    "    for i in range(len(all_tokens) - 1):\n",
    "        bigram.append((all_tokens[i], all_tokens[i + 1]))\n",
    "    bigram_count = Counter(bigram)\n",
    "\n",
    "    for i in bigram:\n",
    "        first_order_condition_prob[i] = (bigram_count[i] + smoothing_alpha) / (word_frequency[i[0]] + smoothing_alpha * V)\n",
    "    \n",
    "    return word_prob_dict, first_order_condition_prob\n",
    "# word_prob_dict, first_order_condition_prob = get_smoothed_first_order_conditional_probabilities(ptb_train, 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "3008240c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def smoothed_bigram_lm_seq_probability(input_sentence: str,\n",
    "                                       word_prob_dict: dict,\n",
    "                                       word_frequency_dict: dict,\n",
    "                                       first_order_condition_prob: dict,\n",
    "                                       smoothing_alpha: float):\n",
    "    '''\n",
    "    args:\n",
    "        input_sentence: The input sequence string. Here we assume\n",
    "        word_prob_dict: a dictionary containing the word probabilities\n",
    "        word_frequency_dict: a dictionary containing the frequency for every word in vocabulary\n",
    "        first_order_condition_prob: a dictionary containing the first order conditional probabilities\n",
    "                                    as discussed in the previous function.\n",
    "        smoothing_alpha: The alpha parameter used in the Laplace smoothing.\n",
    "    output:\n",
    "        probability: The probability of the input_sentence according to the Bi-gram language model\n",
    "    '''\n",
    "    # YOUR CODE HERE\n",
    "    vocab_size = len(word_prob_dict)\n",
    "    token_list = input_sentence.split()\n",
    "    bigram_list = [(s1, s2) for s1, s2 in zip(token_list, token_list[1:])]\n",
    "    probability = np.prod(\n",
    "        [word_prob_dict[token_list[0]]] + [first_order_condition_prob.get(\n",
    "            bigram, smoothing_alpha/(word_frequency_dict[bigram[0]] + smoothing_alpha*vocab_size))\n",
    "                                           for bigram in bigram_list]) # calculates smoothed probability only for unseen bigram\n",
    "    \n",
    "    return probability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd77e6c",
   "metadata": {},
   "source": [
    "#### Assuming $\\alpha=0.01$ for the smoothing, use the previous function and `bigram_lm_seq_probability` to compute the sequence probabilities for all the sentences in the `ptb_test` dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "e2c76c94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0% of samples in the test set have zero probability!\n"
     ]
    }
   ],
   "source": [
    "smoothed_bigram_test_probabilities = []\n",
    "\n",
    "# YOUR CODE HERE\n",
    "word_frequency_dict = Counter(\" \".join([i[\"sentence\"] for i in ptb_train]).split())\n",
    "(word_prob_dict, smoothed_first_order_condition_prob) = get_smoothed_first_order_conditional_probabilities(ptb_train, 0.01)\n",
    "smoothed_bigram_test_probabilities = [\n",
    "    smoothed_bigram_lm_seq_probability(\n",
    "        input_sentence=sample[\"sentence\"],\n",
    "        word_prob_dict=word_prob_dict,\n",
    "        word_frequency_dict=word_frequency_dict,\n",
    "        first_order_condition_prob=smoothed_first_order_condition_prob,\n",
    "        smoothing_alpha=0.01)\n",
    "    for sample in ptb_test]\n",
    "\n",
    "print(f\"{smoothed_bigram_test_probabilities.count(0)/len(ptb_test)*100}% of samples in the test set have zero probability!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f8b38b8",
   "metadata": {},
   "source": [
    "If the perplexity for a given sequence is computed as below, compute the Bigram language model perplexity over `ptb_test` dataset over all the sentences ($\\alpha=0.01)$:\n",
    "$$Perplexity(x_1x_2...x_n) = p(x_1x_2...x_n)^{-1/n}$$\n",
    ", where $p(x_1x_2...x_n)$ is the probability assigned to $x_1x_2...x_n$ sequence by the language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "7e3665f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bigram language model perplexity is 303.67026527743525\n"
     ]
    }
   ],
   "source": [
    "bigram_lm_perplexity = -1\n",
    "# YOUR CODE HERE\n",
    "log_perplex_list = []\n",
    "for idx in range(len(ptb_test)):\n",
    "    sentence_prob = smoothed_bigram_test_probabilities[idx]\n",
    "    if sentence_prob == 0:\n",
    "        continue\n",
    "    sentence_length = len(ptb_test[idx][\"sentence\"].split())\n",
    "    log_perplex_list.append(-np.log2(sentence_prob)/sentence_length)\n",
    "bigram_lm_perplexity = 2**np.mean(log_perplex_list)\n",
    "\n",
    "print(f\"Bigram language model perplexity is {bigram_lm_perplexity}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd753ef",
   "metadata": {},
   "source": [
    "Repeat the same steps but for `cleaned_train_dataset` and `cleaned_test_dataset` datasets where rare tokens (with frequency less than 10) are replaced with `<unk>` token. Do we have a better or a worse perplexity compared to the previous computed perplexity?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "92b98f8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(cleaned) Bigram language model perplexity is 197.99456511856786\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "cleaned_word_frequency_dict = Counter(\" \".join([i[\"sentence\"] for i in cleaned_train_dataset]).split())\n",
    "(cleaned_word_prob_dict,\n",
    "    cleaned_smoothed_first_order_condition_prob) = get_smoothed_first_order_conditional_probabilities(cleaned_train_dataset,\n",
    "                                                                                                      0.01)\n",
    "cleaned_smoothed_bigram_test_probabilities = [\n",
    "    smoothed_bigram_lm_seq_probability(\n",
    "        input_sentence=sample[\"sentence\"],\n",
    "        word_prob_dict=cleaned_word_prob_dict,\n",
    "        word_frequency_dict=cleaned_word_frequency_dict,\n",
    "        first_order_condition_prob=cleaned_smoothed_first_order_condition_prob,\n",
    "        smoothing_alpha=0.01)\n",
    "    for sample in cleaned_test_dataset]\n",
    "\n",
    "\n",
    "# YOUR CODE HERE\n",
    "log_perplex_list = []\n",
    "for idx in range(len(cleaned_test_dataset)):\n",
    "    sentence_prob = cleaned_smoothed_bigram_test_probabilities[idx]\n",
    "    if sentence_prob==0:\n",
    "        continue\n",
    "    sentence_length = len(cleaned_test_dataset[idx][\"sentence\"].split())\n",
    "    log_perplex_list.append(-np.log2(sentence_prob)/sentence_length)\n",
    "cleaned_bigram_lm_perplexity = 2**np.mean(log_perplex_list)\n",
    "\n",
    "print(f\"(cleaned) Bigram language model perplexity is {cleaned_bigram_lm_perplexity}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ea844a",
   "metadata": {},
   "source": [
    "## 1.3 Tri-gram Language Model <a name='trigram_lm'></a>\n",
    "In the Tri-gram language model, we pick/generate next token conditioned only on the two previous tokens. Therefore, for an arbitrary sequence $x_1x_2~...x_n$, its respective probability becomes:\n",
    "$$p(x_1x_2~...x_n) = p(x_1) p(x_2|x_1) ~\\Pi_{i=3} ^n p(x_i|x_{i-2}x_{i-1})$$\n",
    "Let's use the same dataset (`Penn Treebank`) to evaluate this model's perplexity. (We use the dataset that already has the `<stop>` token at the end of each sentence).\n",
    "\n",
    "\n",
    "We estimate $p(x_i|x_{i-1}x_{i-2})$ using the Laplace smoothing with $\\alpha=3 \\cdot 10^{-3}$. First let's write a function that computes these conditional probabilities for the Tri-gram language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abdf0b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_smoothed_second_order_conditional_probabilities(train_dataset: datasets.arrow_dataset.Dataset,\n",
    "                                                        smoothing_alpha: float):\n",
    "    '''\n",
    "    In this function the conditional probabilities have to be computed based on train_dataset. The output\n",
    "    of the function is a dictionary having keys like (x_{i-2}, x_{i-1}, x_i) as a tuple and the\n",
    "    value being p(x_i | x_{i-2} x_{i-1}).\n",
    "    args:\n",
    "        train_dataset: a Dataset object that can be iterated to get all the sentences\n",
    "        smoothing_alpha: The alpha parameter used in the Laplace smoothing.\n",
    "    output:\n",
    "        word_prob_dict: a dictionary containing the word probabilities \n",
    "        first_order_condition_prob: a dictionary containing the smoothed first order\n",
    "                                    conditional probabilities.\n",
    "        second_order_condition_prob: a dictionary containing the smoothed second order\n",
    "                                     conditional probabilities.\n",
    "    '''\n",
    "    smoothed_second_order_condition_prob = defaultdict(float)  # Note that we shouldn't get zeros for unseen probabilies.\n",
    "    \n",
    "    # let's first get the 0th and 1st order conditional probabilities\n",
    "    (word_prob_dict, first_order_condition_prob) = get_smoothed_first_order_conditional_probabilities(\n",
    "        train_dataset, smoothing_alpha)\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    return word_prob_dict, first_order_condition_prob, second_order_condition_prob\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "936a664f",
   "metadata": {},
   "source": [
    "#### Now let's analyze the Tri-gram language model for different sequences. We first create a function that can output the probability for a given string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f6a692c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def smoothed_trigram_lm_seq_probability(input_sentence: str,\n",
    "                                        word_prob_dict: dict,\n",
    "                                        word_frequency_dict: dict,\n",
    "                                        bigram_frequency_dict: dict,\n",
    "                                        first_order_condition_prob: dict,\n",
    "                                        second_order_condition_prob: dict,\n",
    "                                        smoothing_alpha: float):\n",
    "    '''\n",
    "    args:\n",
    "        input_sentence: The input sequence string. Here we assume\n",
    "        word_prob_dict: a dictionary containing the word probabilities\n",
    "        word_frequency_dict: a dictionary containing the frequency for every word in vocabulary\n",
    "        bigram_frequency_dict: a dictionary containing the frequency for every bigram in vocabulary\n",
    "        first_order_condition_prob: a dictionary containing the first order conditional probabilities\n",
    "                                    as discussed earlier.\n",
    "        second_order_condition_prob: a dictionary containing the second order conditional probabilities\n",
    "                                     as discussed in the previous function.\n",
    "    output:\n",
    "        probability: The probability of the input_sentence according to the Bi-gram language model\n",
    "    '''\n",
    "\n",
    "    # YOUR CODE HERE    \n",
    "\n",
    "    return probability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16865742",
   "metadata": {},
   "source": [
    "#### Now let's compute the probability for sequences in the test dataset, assuming $\\alpha=3\\cdot10^{-3}$ has been used in the Laplace smoothing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36feefd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "smoothed_trigram_test_probabilities = []\n",
    "\n",
    "# YOUR CODE HERE\n",
    "\n",
    "print(f\"{smoothed_trigram_test_probabilities.count(0)/len(ptb_test)*100}% of samples in the test set have zero probability!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ddd040",
   "metadata": {},
   "source": [
    "Now we compute the perplexity on the `ptb_test` dataset for the tri-gram language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ed3d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "Trigram_lm_perplexity = -1\n",
    "# YOUR CODE HERE\n",
    "\n",
    "print(f\"Trigram language model perplexity is {Trigram_lm_perplexity}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e34c59c8",
   "metadata": {},
   "source": [
    "Repeat the same steps but for `cleaned_train_dataset` and `cleaned_test_dataset` datasets where rare tokens (with frequency less than 10) are replaced with `<unk>` token. Do we have a better or a worse perplexity compared to the previous computed perplexity?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b456be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d8e69cc",
   "metadata": {},
   "source": [
    "#### Discussion\n",
    " - How are the three discussed models performance compare to each other?\n",
    " - What is the cost of using N-gram language models for even larger N values?\n",
    " - What is the effect of vocabulary size on models' perplexities? Can we compare models with different vocabulary sizes?\n",
    " - What is the perplexity of a language model (vocabulary size of |V|) that given any context (i.e., $x_1 x_2 ... x_{n-1}$) assigns uniform probabilities (for all the tokens in the vocabulary) for the next token? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "641578b1",
   "metadata": {},
   "source": [
    "## 2. Task B: Neural Language Models <a name='neural_lm'></a>\n",
    "\n",
    "In this exercise, we will better understand the functioning of some simple neural language models. We first start with a fixed-window neural language model. In the following subsection, we will investigate an RNN-based language model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88cee765",
   "metadata": {},
   "source": [
    "### 2.1 Fixed-Window Neural Language Model <a name='fixed_window_lm'></a>\n",
    "This language model take as input a constant number of tokens, and then outputs a probability distribution for the next token. In this section, we assume the underlying model is a Multi-layer Perceptron (MLP) with a single hidden layer. This model doesn't have the sparsity issue of N-gram language models, but is always limited to a fixed window of tokens.\n",
    "\n",
    "In this section, we don't include the training of the model but rather we use a pretrained model on the same training dataset. We evaluate the language model over the `ptb_test` dataset, to show the power of neural language models, when compared to N-gram language models.\n",
    "\n",
    "More importantly, we use PyTorch modules in this section, so that you get more familiar with its capabilities. Throughout this exercise, we use a `window_size=3` for this model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6808da1e",
   "metadata": {},
   "source": [
    "Let's first create a dataset of all consecutive tokens of length `window_size` from the `ptb_train` dataset. you can read more about PyTorch datasets and how to create a custom dataset  [here](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html#creating-a-custom-dataset-for-your-files)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f5c3bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "window_size = 3\n",
    "vocabulary_size = 10000\n",
    "word_emb_dim = 100\n",
    "hidden_dim = 100\n",
    "\n",
    "\n",
    "class FixedWindowDataset(Dataset):\n",
    "    # read more about custom datasets at https://pytorch.org/tutorials/beginner/basics/data_tutorial.html\n",
    "    def __init__(self,\n",
    "                 train_dataset: datasets.arrow_dataset.Dataset,\n",
    "                 test_dataset: datasets.arrow_dataset.Dataset,\n",
    "                 window_size: int,\n",
    "                 vocabulary_size: int\n",
    "                ):\n",
    "        self.prepared_train_dataset = self.prepare_fixed_window_lm_dataset(train_dataset, window_size + 1)\n",
    "        self.prepared_test_dataset = self.prepare_fixed_window_lm_dataset(test_dataset, window_size + 1)\n",
    "        \n",
    "        dataset_vocab = self.get_dataset_vocabulary(train_dataset)\n",
    "        # defining a dictionary that simply maps tokens to their respective index in the embedding matrix\n",
    "        self.word_to_index = {word: idx for idx,word in enumerate(dataset_vocab)}\n",
    "        self.index_to_word = {idx: word for idx,word in enumerate(dataset_vocab)}\n",
    "        \n",
    "        assert vocabulary_size >= len(dataset_vocab) , f\"The dataset vocab size is {len(dataset_vocab)}!\"\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.prepared_train_dataset)\n",
    "    \n",
    "    def get_encoded_test_samples(self):\n",
    "        all_token_lists = [sample.split() for sample in self.prepared_test_dataset]\n",
    "        all_token_ids = [[self.word_to_index.get(word, self.word_to_index[\"<unk>\"])\n",
    "                          for word in token_list[:-1]]\n",
    "                         for token_list in all_token_lists\n",
    "                        ]\n",
    "        all_next_token_ids = [self.word_to_index.get(token_list[-1], self.word_to_index[\"<unk>\"]) for \n",
    "                              token_list in all_token_lists]\n",
    "        return torch.tensor(all_token_ids), torch.tensor(all_next_token_ids)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        # here we need to transform the data to the format we expect at the model input\n",
    "        token_list = self.prepared_train_dataset[idx].split()\n",
    "        # having a fallback to <unk> token if an unseen word is encoded.\n",
    "        token_ids = [self.word_to_index.get(word, self.word_to_index[\"<unk>\"]) for word in token_list[:-1]]\n",
    "        next_token_id = self.word_to_index.get(token_list[-1], self.word_to_index[\"<unk>\"])\n",
    "        return torch.tensor(token_ids), torch.tensor(next_token_id)\n",
    "    \n",
    "    def decode_idx_to_word(self, token_id):\n",
    "        return [self.index_to_word[id_.item()] for id_ in token_id]\n",
    "    \n",
    "    def get_dataset_vocabulary(self, train_dataset: datasets.arrow_dataset.Dataset):\n",
    "        vocab = sorted(set(\" \".join([sample[\"sentence\"] for sample in train_dataset]).split()))\n",
    "        # we also add a <start> token to include initial tokens in the sentences in the dataset\n",
    "        vocab += [\"<start>\"]\n",
    "        return vocab\n",
    "    \n",
    "    @staticmethod\n",
    "    def prepare_fixed_window_lm_dataset(target_dataset: datasets.arrow_dataset.Dataset,\n",
    "                                        window_size: int):\n",
    "        '''\n",
    "        Please note that for the very first tokens, they will be added like \"<start> <start> Token#1\".\n",
    "        args:\n",
    "            target_dataset: the target dataset where its consecutive tokens of length 'window_size' should be extracted\n",
    "            window_size: the window size for the language model\n",
    "        output:\n",
    "            prepared_dataset: a list of strings each containing 'window_size' tokens.\n",
    "        '''\n",
    "        \n",
    "        prepared_dataset = []\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        \n",
    "        return prepared_dataset\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "524f4a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_window_dataset = FixedWindowDataset(ptb_train, ptb_test, window_size, vocabulary_size)\n",
    "\n",
    "# let's create a simple dataloader for this dataset\n",
    "train_dataloader =  DataLoader(fixed_window_dataset, batch_size=8, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b44e90d",
   "metadata": {},
   "source": [
    "Now, let's define the underlying PyTorch model for the language model. You can read more about PyTorch models [here](https://pytorch.org/tutorials/beginner/introyt/modelsyt_tutorial.html).\n",
    "\n",
    "**Note**: Here in the forward pass, we compute the negative log-likelihood after passing through the MLP layers. Here we use `torch.nn.LogSoftmax`, as it's numerically more stable than doing seperately `softmax` followed by taking its logarithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ea691f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "class Fixed_window_language_model(torch.nn.Module):\n",
    "    def __init__(self, emb_dim, hidden_dim, window_size, vocab_size=10000):\n",
    "        super().__init__()\n",
    "\n",
    "        self.window_size = window_size\n",
    "        self.emb_dim = emb_dim\n",
    "        self.word_embeddings = torch.nn.Embedding(vocab_size, emb_dim) # word embeddings\n",
    "        self.linear1 = torch.nn.Linear(window_size * emb_dim, hidden_dim) # first linear layer\n",
    "        self.activation_func = torch.tanh # the activation function\n",
    "        self.linear2 = torch.nn.Linear(hidden_dim, vocab_size) # second linear layer\n",
    "        \n",
    "        self.log_softmax = torch.nn.LogSoftmax(dim=1)\n",
    "        self.criterion = torch.nn.NLLLoss()\n",
    "     \n",
    "    def forward(self, input_ids, labels):\n",
    "        inputs_embeds = self.word_embeddings(input_ids)\n",
    "        concat_input_embed = inputs_embeds.reshape(-1, self.emb_dim * self.window_size)\n",
    "        hidden_state = self.activation_func( self.linear1(concat_input_embed) )\n",
    "        logits = self.log_softmax( self.linear2(hidden_state) )\n",
    "        loss = self.criterion(logits, labels)\n",
    "        \n",
    "        return loss\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dae5397",
   "metadata": {},
   "source": [
    "Now let's see how easy it is to train a model with PyTorch! (we provide a trained model in the cell after train, so that you can just start using the model without going through the time-consuming training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93779901",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the model\n",
    "model_fixed_window = Fixed_window_language_model(emb_dim=word_emb_dim, hidden_dim=hidden_dim,\n",
    "                                                 window_size=window_size, vocab_size=vocabulary_size)\n",
    "\n",
    "# defining the optimizer\n",
    "optimizer = optim.SGD(model_fixed_window.parameters(),\n",
    "                      lr=0.005,\n",
    "                      momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77626d29",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for epoch in range(2):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_dataloader):\n",
    "        # get the inputs; data is a tuple of (context, target)\n",
    "        context, target = data\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        loss = model_fixed_window(context, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 5000 == 4999. :    # print every 5000 mini-batches\n",
    "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 5000:.3f}')\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')\n",
    "\n",
    "# saving the trained model\n",
    "torch.save(model_fixed_window.state_dict(), \"fixed_window_model.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0483dd01",
   "metadata": {},
   "source": [
    "We provide a trained model, so that you can start using it right away"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ba51b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_window_checkpoint_file = \"fixed_window_model.pt\"\n",
    "model_fixed_window.load_state_dict(torch.load(fixed_window_checkpoint_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b806f375",
   "metadata": {},
   "outputs": [],
   "source": [
    "# context and 'target' ids (target is the next word after the context)\n",
    "test_token_ids, test_target_ids = fixed_window_dataset.get_encoded_test_samples()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f87e283d",
   "metadata": {},
   "source": [
    "We now have the `test_token_ids`, `test_target_ids` tensors for the test dataset. The `test_token_ids` are the context ids and `test_target_ids` are the respective **next token** (a.k.a. target here) for these contexts.\n",
    "#### Using the trained model, implement a function that can output the loss for the discussed test dataset. How can we generally decide if the model is overfitted to the train dataset or not?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e352e019",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_test_dataset_loss(model: torch.nn.Module,\n",
    "                               test_token_ids: torch.Tensor,\n",
    "                               test_target_ids: torch.Tensor):\n",
    "    '''\n",
    "    args:\n",
    "        model: fixed-window language model\n",
    "        test_token_ids: the context ids in a single tensor.\n",
    "        test_target_ids: the target ids (next token after the context) in a single tensor.\n",
    "    output:\n",
    "        avg_test_loss: The average loss of model over test dataset.\n",
    "    '''\n",
    "    batch_size = 4\n",
    "    test_loss = []\n",
    "    \n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    return avg_test_loss\n",
    "\n",
    "\n",
    "test_dataset_loss = generate_test_dataset_loss(model_fixed_window, test_token_ids, test_target_ids)\n",
    "print(f\"Test dataset loss is {test_dataset_loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "085d7476",
   "metadata": {},
   "source": [
    "#### Using the trained fixed-window model, implement a function that can output entropy for a given sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db54e454",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_seqeuence_entropy_fixed_window_lm(model: torch.nn.Module,\n",
    "                                              input_sequence: str,\n",
    "                                              window_size: int,\n",
    "                                              word_to_idx: dict):\n",
    "    '''\n",
    "    Note that e.g., in order to get the first token probability, you need to pass a sequence\n",
    "    like \"<start> <start> <start>\" (prefix padding) to the neural model. In a similar fashion, we need to pass\n",
    "    \"<start> <start> TOKEN#1\" for getting the probability of the second token.\n",
    "    args:\n",
    "        model: fixed-window language model\n",
    "        input_sequence: the sequence for which we want to calculate the probability\n",
    "        window_size: the size of window for the language model\n",
    "        word_to_idx: a mapping from words to the embedding indices (to encode tokens before being\n",
    "                     passed to model). You can get this dict from 'fixed_window_dataset.word_to_index'\n",
    "    output:\n",
    "        sequence_entropy: the entropy for the input sequence using the trained model\n",
    "    '''\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "\n",
    "    return sequence_entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d055835",
   "metadata": {},
   "source": [
    "#### Compute the perplexity for the trained fixed-window language model over `ptb_test` dataset using the previous function. How does it perform compared to N-gram language models we discussed earlier?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4289886",
   "metadata": {},
   "outputs": [],
   "source": [
    "perplexity = -1\n",
    "\n",
    "# YOUR CODE HERE\n",
    "\n",
    "print(f\"The fixed-window model perplexity over test dataset is {perplexity}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "240c3d89",
   "metadata": {},
   "source": [
    "### 2.2 RNN-based Language Model <a name='rnn_lm'></a>\n",
    "To address the need for a neural architecture that can proceed with any length input (as opposed to the fixed-window model that can only process a fixed number of tokens), we implement the Recurrent Neural Network (RNN). The core idea behind is that we can apply the same weight W repeatedly.\n",
    "\n",
    "An advatange of RNN model compared to fixed-window langauage model is that we can pass a given sentence at once, instead of passing it in many windows of size `window_size`. Moreover, the language model has the ability to look behind further that a fixed number of tokens.\n",
    "\n",
    " As we already did a neural model training exercise for the previous neural model, we only provide a trained LM at this section, so that you can focus only on the analysis part.\n",
    " \n",
    "You can find the dataset structure as well as the RNN architecture in the `rnn_utils.py` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc631bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rnn_utils import RNNDataset, RNN_language_model\n",
    "\n",
    "vocabulary_size = 10000\n",
    "word_emb_dim = 200\n",
    "hidden_dim = 200\n",
    "\n",
    "rnn_dataset = RNNDataset(ptb_train, ptb_test, vocabulary_size)\n",
    "\n",
    "# if gpu is available, we puts the model on it \n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Here we need a <pad> token for the RNN model, in order to have a batch of sequences with difference sizes \n",
    "pad_idx = rnn_dataset.pad_idx # the index for <pad> token\n",
    "rnn_model = RNN_language_model(vocab_size=vocabulary_size, emb_dim=word_emb_dim, hidden_dim=hidden_dim,\n",
    "                               pad_idx=pad_idx)\n",
    "rnn_model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25137a9f",
   "metadata": {},
   "source": [
    "load the model weights using the state_dict in `rnn_model.pt` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52adb1f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f06eb967",
   "metadata": {},
   "source": [
    "As the training of an RNN model is time-consuming, we provide a trained language model on this dataset (`rnn_model.pt`), so that you can just analyze the model performance here.\n",
    "As mentioned above, as RNN can get sequences with varying lengths, the input sequences should be padded with a special token like `<pad>`, so that we can create a batch of sentences. The output of the defined RNN model (see the architecture detail `rnn_utils.py`) is the model's entropy over the input data.\n",
    "\n",
    "#### First get the encoded test samples of `ptb_test` dataset, and then pass these (already padded) sentences to the RNN model to get the respective entropy values. Compute the perplexity of the model and compare it with previous approaches.\n",
    "**HINT**: You can use the `get_encoded_test_samples` function of `rnn_dataset` to get encoded test samples.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d7b99f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_perplexity = -1\n",
    "\n",
    "# YOUR CODE HERE\n",
    "\n",
    "print(f\"The model perplexity is {test_perplexity}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "a8ced340a52f9326f5856e1d63a73f97bd9f0a225610b549ff7b502d766a19ce"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
